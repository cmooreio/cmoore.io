import{_ as e,c as a,o as r,ag as o}from"./chunks/framework.CI6Ra_OS.js";const g=JSON.parse('{"title":"AI Inference","description":"","frontmatter":{"prev":{"text":"Observability","link":"/components/observability"},"next":{"text":"Hardware","link":"/hardware/"}},"headers":[],"relativePath":"components/ai-inference.md","filePath":"components/ai-inference.md"}'),n={name:"components/ai-inference.md"};function d(i,t,s,h,l,c){return r(),a("div",null,[...t[0]||(t[0]=[o('<h1 id="ai-inference" tabindex="-1">AI Inference <a class="header-anchor" href="#ai-inference" aria-label="Permalink to &quot;AI Inference&quot;">​</a></h1><p>Local LLM inference using a multi-pod architecture with dedicated GPU nodes for both AMD ROCm and NVIDIA CUDA workloads.</p><h2 id="architecture" tabindex="-1">Architecture <a class="header-anchor" href="#architecture" aria-label="Permalink to &quot;Architecture&quot;">​</a></h2><p>The AI server uses a multi-pod design where each component runs as a separate deployment, all sharing model files via <code>hostPath</code> mounts to <code>/data/ai-models</code>.</p><table tabindex="0"><thead><tr><th>Pod</th><th>Node</th><th>Purpose</th></tr></thead><tbody><tr><td><strong>ROCm</strong></td><td>aimax</td><td>llama.cpp with AMD ROCm GPU</td></tr><tr><td><strong>Thor</strong></td><td>thor</td><td>vLLM, ComfyUI, Speaches TTS</td></tr><tr><td><strong>LiteLLM</strong></td><td>any</td><td>OpenAI-compatible API gateway</td></tr><tr><td><strong>Open WebUI</strong></td><td>any</td><td>Chat interface (sidecar to LiteLLM)</td></tr></tbody></table><h2 id="rocm-pod" tabindex="-1">ROCm Pod <a class="header-anchor" href="#rocm-pod" aria-label="Permalink to &quot;ROCm Pod&quot;">​</a></h2><p>Runs <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noreferrer">llama.cpp</a> with ROCm GPU acceleration on the Minisforum MS-S1 Max (Ryzen AI Max+ 395). This node handles the bulk of LLM inference with 128GB of unified memory.</p><table tabindex="0"><thead><tr><th>Setting</th><th>Value</th></tr></thead><tbody><tr><td><strong>Node</strong></td><td>aimax</td></tr><tr><td><strong>Taint</strong></td><td><code>rocm-inference=true:NoSchedule</code></td></tr><tr><td><strong>GPU</strong></td><td>AMD Radeon integrated (ROCm)</td></tr><tr><td><strong>Models</strong></td><td>Served from <code>/data/ai-models</code> hostPath</td></tr></tbody></table><h2 id="thor-pod" tabindex="-1">Thor Pod <a class="header-anchor" href="#thor-pod" aria-label="Permalink to &quot;Thor Pod&quot;">​</a></h2><p>Runs on the NVIDIA AGX Thor Jetson node with CUDA acceleration:</p><ul><li><strong><a href="https://docs.vllm.ai/" target="_blank" rel="noreferrer">vLLM</a></strong> - High-throughput LLM serving</li><li><strong><a href="https://github.com/comfyanonymous/ComfyUI" target="_blank" rel="noreferrer">ComfyUI</a></strong> - Image generation workflows</li><li><strong><a href="https://github.com/speaches-ai/speaches" target="_blank" rel="noreferrer">Speaches</a></strong> - Text-to-speech and transcription</li></ul><table tabindex="0"><thead><tr><th>Setting</th><th>Value</th></tr></thead><tbody><tr><td><strong>Node</strong></td><td>thor</td></tr><tr><td><strong>Taint</strong></td><td><code>cuda-inference=true:NoSchedule</code></td></tr><tr><td><strong>GPU</strong></td><td>NVIDIA Blackwell (CUDA)</td></tr><tr><td><strong>Memory</strong></td><td>128GB</td></tr></tbody></table><h2 id="litellm" tabindex="-1">LiteLLM <a class="header-anchor" href="#litellm" aria-label="Permalink to &quot;LiteLLM&quot;">​</a></h2><p><a href="https://litellm.ai/" target="_blank" rel="noreferrer">LiteLLM</a> acts as a unified API gateway, exposing all models from both ROCm and CUDA backends through a single <strong>OpenAI-compatible API</strong> endpoint. Applications can switch between models and backends without changing API calls.</p><h2 id="open-webui" tabindex="-1">Open WebUI <a class="header-anchor" href="#open-webui" aria-label="Permalink to &quot;Open WebUI&quot;">​</a></h2><p><a href="https://openwebui.com/" target="_blank" rel="noreferrer">Open WebUI</a> provides a ChatGPT-style web interface for interacting with local models. It runs as a sidecar alongside LiteLLM, connecting through the local API gateway.</p>',16)])])}const m=e(n,[["render",d]]);export{g as __pageData,m as default};
